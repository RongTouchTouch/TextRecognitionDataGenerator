{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import shutil\n",
    "import contextlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from utils.converter import LabelConverter, IndexConverter\n",
    "from datasets.dataset import InMemoryDigitsDataset, DigitsDataset, collate_train, collate_dev, inmemory_train, inmemory_dev\n",
    "from generate import gen_text_img\n",
    "\n",
    "import arguments\n",
    "import models\n",
    "from models.crnn import init_network\n",
    "from models.densenet_ import DenseNet\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"always\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    for i, sample in enumerate(train_loader):\n",
    "        # Measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        # Zero out gradients so we can accumulate new ones over batches\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # step 2. Get our inputs targets ready for the network.\n",
    "        # targets is a list of `torch.IntTensor` with `batch_size` size.\n",
    "        target_lengths = sample.target_lengths.to(device)\n",
    "        targets = sample.targets # Expected targets to have CPU Backend\n",
    "\n",
    "        # step 3. Run out forward pass.\n",
    "        images = sample.images\n",
    "        print(images.size())\n",
    "        if isinstance(images, tuple):\n",
    "            targets = targets.to(device)\n",
    "            log_probs = []\n",
    "            for image in images:\n",
    "                image = image.unsqueeze(0).to(device)\n",
    "                log_prob = model(image).squeeze(1)\n",
    "                log_probs.append(log_prob)\n",
    "            input_lengths = torch.IntTensor([i.size(0) for i in log_probs]).to(device)\n",
    "            log_probs = pad_sequence(log_probs)\n",
    "        else: # Batch\n",
    "            images = images.to(device)\n",
    "            log_probs = model(images)\n",
    "            input_lengths = torch.full((images.size(0),), log_probs.size(0), dtype=torch.int32, device=device)\n",
    "        \n",
    "        # step 4. Compute the loss, gradients, and update the parameters\n",
    "        # by calling optimizer.step()\n",
    "        print(targets.shape)\n",
    "        loss = criterion(log_probs, targets, input_lengths, target_lengths)\n",
    "        losses.update(loss.item())\n",
    "        loss.backward()\n",
    "\n",
    "        # do one step for multiple batches\n",
    "        # accumulated gradients are used\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if (i+1) % args.print_freq == 0 or i == 0 or (i+1) == len(train_loader):\n",
    "            print('>> Train: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})'.format(\n",
    "                   epoch+1, i+1, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses))\n",
    "\n",
    "    return losses.avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(dev_loader, model, epoch, converter):\n",
    "    batch_time = AverageMeter()\n",
    "    accuracy = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    num_correct = 0\n",
    "    num_verified = 0\n",
    "    end = time.time()\n",
    "\n",
    "    #for i, (images, targets) in enumerate(dev_loader):\n",
    "    for i, sample in enumerate(dev_loader):\n",
    "        images = sample.images\n",
    "        targets = sample.targets\n",
    "        if isinstance(images, tuple):\n",
    "            preds = []\n",
    "            for image in images:\n",
    "                image = image.unsqueeze(0).to(device)\n",
    "                log_prob = model(image)\n",
    "                preds.append(converter.best_path_decode(log_prob, strings=False))\n",
    "        else: # Batch\n",
    "            images = images.to(device)\n",
    "            log_probs = model(images)\n",
    "            preds = converter.best_path_decode(log_probs, strings=False)\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        for i in range(len(targets)):\n",
    "             num_verified += len(targets[i])\n",
    "        for pred, target in zip(preds, targets):\n",
    "            print(pred)\n",
    "            print(target)\n",
    "            temp = num_correct\n",
    "            for i in range(min(len(target),len(pred))):\n",
    "                if target[i]==pred[i]:\n",
    "                    num_correct += 1\n",
    "#                 elif len(target)>len(pred):\n",
    "#                     if target[i+1]==pred[i]:\n",
    "#                         num_correct += 1\n",
    "#                 elif len(target)<len(pred):\n",
    "#                     if target[i]==pred[i+1]:\n",
    "#                         num_correct += 1\n",
    "            print(num_correct-temp)\n",
    "        accuracy.update(num_correct / num_verified) # character\n",
    "        \n",
    "#         batch_time.update(time.time() - end)\n",
    "#         end = time.time()\n",
    "#         num_verified += len(targets)\n",
    "#         for pred, target in zip(preds, targets):\n",
    "#             if pred == target:\n",
    "#                 num_correct += 1\n",
    "#         accuracy.update(num_correct / num_verified) # character\n",
    "        \n",
    "        for i in range(len(preds)):\n",
    "            pred = converter.best_path_decode(log_probs, strings=True)[i].decode('utf-8')\n",
    "            print('pred: {}'.format(pred))\n",
    "\n",
    "        if (i+1) % args.print_freq == 0 or i == 0 or (i+1) == len(dev_loader):\n",
    "            print('>> Val: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Accu {accuracy.val:.3f}'.format(\n",
    "                   epoch+1, i+1, len(dev_loader), batch_time=batch_time, accuracy=accuracy))\n",
    "\n",
    "    return accuracy.val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, directory):\n",
    "    filename = os.path.join(directory, '{}_epoch_{}.pth.tar'.format(state['arch'], state['epoch']))\n",
    "    with contextlib.suppress(FileNotFoundError):\n",
    "        os.remove(filename)\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        print('>>>> save best model at epoch: {}'.format(state['epoch']))\n",
    "        filename_best = os.path.join(directory, '{}_best.pth.tar'.format(state['arch']))\n",
    "        with contextlib.suppress(FileNotFoundError):\n",
    "            os.remove(filename_best)\n",
    "        shutil.copyfile(filename, filename_best)\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def set_batchnorm_eval(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('BatchNorm') != -1:\n",
    "        # freeze running mean and std:\n",
    "        # we do training one image at a time\n",
    "        # so the statistics would not be per batch\n",
    "        # hence we choose freezing (ie using imagenet statistics)\n",
    "        m.eval()\n",
    "        # # freeze parameters:\n",
    "        # # in fact no need to freeze scale and bias\n",
    "        # # they can be learned\n",
    "        # # that is why next two lines are commented\n",
    "        # for p in m.parameters():\n",
    "            # p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# alphabet/alphabet_decode_5990.txt\n",
    "sys.argv = ['main.py','--dataset-root','alphabet','--arch','densenet121','--alphabet','alphabet/alphabet_decode_5990.txt',\n",
    "            '--lr','5e-5','--max-epoch','100','--optimizer','rmsprop','--gpu-id','-1','--not-pretrained']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Creating directory if it does not exist:\n",
      ">> './checkpoint/densenet121_rmsprop_lr5.0e-05_wd5.0e-04_bsize64_imsize32'\n",
      ">> Using model from scratch (random weights) 'densenet121'\n"
     ]
    }
   ],
   "source": [
    "args = arguments.parse_args()\n",
    "\n",
    "if args.gpu_id < 0:\n",
    "    device = torch.device(\"cpu\")\n",
    "else:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu_id)\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# create export dir if it doesnt exist\n",
    "directory = \"{}\".format(args.arch)\n",
    "directory += \"_{}_lr{:.1e}_wd{:.1e}\".format(args.optimizer, args.lr, args.weight_decay)\n",
    "directory += \"_bsize{}_imsize{}\".format(args.batch_size, args.image_size)\n",
    "\n",
    "args.directory = os.path.join(args.directory, directory)\n",
    "print(\">> Creating directory if it does not exist:\\n>> '{}'\".format(args.directory))\n",
    "if not os.path.exists(args.directory):\n",
    "    os.makedirs(args.directory)\n",
    "\n",
    "# initialize model\n",
    "if args.pretrained:\n",
    "    print(\">> Using pre-trained model '{}'\".format(args.arch))\n",
    "else:\n",
    "    print(\">> Using model from scratch (random weights) '{}'\".format(args.arch))\n",
    "\n",
    "# load alphabet from file\n",
    "if os.path.isfile(args.alphabet):\n",
    "    alphabet = ''\n",
    "    with open(args.alphabet, mode='r', encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            alphabet += line.strip()\n",
    "    args.alphabet = alphabet\n",
    "\n",
    "model_params = {}\n",
    "model_params['architecture'] = args.arch\n",
    "model_params['num_classes'] = len(args.alphabet) + 1\n",
    "model_params['mean'] = (0.5,)\n",
    "model_params['std'] = (0.5,)\n",
    "model_params['pretrained'] = args.pretrained\n",
    "model = init_network(model_params)\n",
    "model = model.to(device)\n",
    "\n",
    "if args.pretrained:\n",
    "# pretrained/densenet121_pretrained.pth\n",
    "    model_path = 'pretrained/params_6.pth'\n",
    "    checkpoint = torch.load(model_path,map_location = 'cpu')\n",
    "    model.load_state_dict(checkpoint)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 280)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.dataset import InMemoryDigitsDataset, DigitsDataset, collate_train, collate_dev, inmemory_train, inmemory_dev\n",
    "\n",
    "num = 100\n",
    "dev_num = num\n",
    "use_file = 1\n",
    "text = \"今天是五月二十二日晴\"\n",
    "text_length = 10\n",
    "font_size = 32\n",
    "font_id = 1\n",
    "space_width = 1\n",
    "text_color = '#282828'\n",
    "thread_count = 8\n",
    "channel = 3\n",
    "\n",
    "random_skew = True\n",
    "skew_angle = 1\n",
    "random_blur = True\n",
    "blur = 0.5\n",
    "\n",
    "orientation = 0\n",
    "distorsion = -1\n",
    "distorsion_orientation = 0\n",
    "background = 1\n",
    "\n",
    "random_process = False\n",
    "noise = 20\n",
    "erode = 2\n",
    "dilate = 2\n",
    "incline = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function\n",
    "criterion = nn.CTCLoss()\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "# define optimizer\n",
    "if args.optimizer == 'sgd':\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "elif args.optimizer == 'rmsprop':\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "elif args.optimizer == 'adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "converter = LabelConverter(args.alphabet, ignore_case=False)\n",
    "\n",
    "# define learning rate decay schedule\n",
    "# TODO: maybe pass as argument in future implementation?\n",
    "exp_decay = math.exp(-0.1)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=exp_decay)\n",
    "\n",
    "is_best = False\n",
    "best_accuracy = 0.0\n",
    "accuracy = 0.0\n",
    "start_epoch = 0\n",
    "\n",
    "acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 32, 280])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [24, 3, 3, 3], expected input[64, 1, 32, 280] to have 3 channels, but got 1 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d4c9429705f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# train for one epoch on train set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;31m# evaluate on validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-e6cb7fb3563e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, criterion, optimizer, epoch)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0minput_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Newmodel/TextRecognitionDataGenerator/models/crnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# x -> features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;31m# features -> pool -> flatten -> classifier -> softmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 320\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [24, 3, 3, 3], expected input[64, 1, 32, 280] to have 3 channels, but got 1 channels instead"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    text_meta, text_img = gen_text_img(num, use_file, text, text_length, font_size, font_id, space_width, background, text_color,\n",
    "                                      orientation, blur, random_blur, distorsion, distorsion_orientation, skew_angle, random_skew,\n",
    "                                      random_process, noise, erode, dilate, incline,\n",
    "                                      thread_count, channel)\n",
    "    #     dev_meta, dev_img = gen_text_img(dev_num, use_file, text, text_length, font_size, font_id, space_width, background, text_color,\n",
    "    #                               orientation, blur, random_blur, distorsion, distorsion_orientation, skew_angle, random_skew,\n",
    "    #                               thread_count)\n",
    "    dev_meta, dev_img = text_meta, text_img\n",
    "\n",
    "    index_converter = IndexConverter(args.alphabet, ignore_case=False)\n",
    "\n",
    "    train_dataset = InMemoryDigitsDataset(mode='train',text=text_meta,img=text_img,total=num,\n",
    "                                      transform=transform, converter = index_converter)\n",
    "    dev_dataset = InMemoryDigitsDataset(mode='dev', text=dev_meta, img=dev_img, total=dev_num,\n",
    "                                    transform=transform, converter = index_converter)\n",
    "\n",
    "    train_loader = data.DataLoader(train_dataset, batch_size=args.batch_size, collate_fn=collate_train,\n",
    "                               shuffle=True, num_workers=args.workers, pin_memory=True)\n",
    "    dev_loader = data.DataLoader(dev_dataset, batch_size=args.batch_size, collate_fn=collate_dev,\n",
    "                             shuffle=False, num_workers=args.workers, pin_memory=True)\n",
    "    \n",
    "    is_best = False\n",
    "    best_accuracy = 0.0\n",
    "    accuracy = 0.0\n",
    "    start_epoch = 0\n",
    "    \n",
    "    for epoch in range(start_epoch, args.max_epoch):\n",
    "        if args.test_only:\n",
    "            args.max_epoch = 1\n",
    "            print('>>>> Test model, using model at epoch: {}'.format(start_epoch))\n",
    "            start_epoch -= 1\n",
    "            with torch.no_grad():\n",
    "                accuracy = validate(dev_loader, model, start_epoch, converter)\n",
    "            acc.append(format(accuracy))\n",
    "            print('>>>> Accuracy: {}'.format(accuracy))\n",
    "        else:\n",
    "            # aujust learning rate for each epoch\n",
    "            scheduler.step()\n",
    "\n",
    "            # train for one epoch on train set\n",
    "            loss = train(train_loader, model, criterion, optimizer, epoch)\n",
    "\n",
    "            # evaluate on validation set\n",
    "            if (epoch + 1) % args.validate_interval == 0:\n",
    "                with torch.no_grad():\n",
    "                    accuracy = validate(dev_loader, model, epoch, converter)\n",
    "\n",
    "            # # evaluate on test datasets every test_freq epochs\n",
    "            # if (epoch + 1) % args.test_freq == 0:\n",
    "            #     with torch.no_grad():\n",
    "            #         test(args.test_datasets, model)\n",
    "\n",
    "            # remember best accuracy and save checkpoint\n",
    "            is_best = accuracy > 0.0 and accuracy >= best_accuracy\n",
    "            best_accuracy = max(accuracy, best_accuracy)\n",
    "            acc.append(format(accuracy))\n",
    "            print('>>>> Accuracy: {}'.format(accuracy))\n",
    "\n",
    "            if (epoch + 1) % args.save_interval == 0:\n",
    "                save_checkpoint({\n",
    "                    'arch': args.arch,\n",
    "                    'epoch': epoch + 1,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'best_accuracy': best_accuracy,\n",
    "\n",
    "\n",
    "                    'optimizer' : optimizer.state_dict(),\n",
    "                }, is_best, args.directory)\n",
    "            \n",
    "            if best_accuracy == 1:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'pretrained/new_prarams.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= [i for i in range(0,len(acc))]\n",
    "plt.plot(x,acc,color = 'b',label=\"weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = list(model.parameters())\n",
    "# k = 0\n",
    "# count = 0\n",
    "# for i in params:\n",
    "#     print('第',count,\"层：\")\n",
    "#     l = 1\n",
    "#     print(\"该层的结构：\" + str(list(i.size())))\n",
    "#     for j in i.size():\n",
    "#         l *= j\n",
    "#     print(\"该层参数和：\" + str(l))\n",
    "#     k = k + l\n",
    "#     count = count+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
