{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import shutil\n",
    "import contextlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from utils.converter import LabelConverter, IndexConverter\n",
    "from datasets.dataset import InMemoryDigitsDataset, DigitsDataset, collate_train, collate_dev, inmemory_train, inmemory_dev\n",
    "from generate import gen_text_img\n",
    "\n",
    "import models\n",
    "from models.crnn import init_network\n",
    "from models.densenet_ import DenseNet\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"always\")\n",
    "\n",
    "\n",
    "# from tensorboardX import SummaryWriter\n",
    "# writer = SummaryWriter('./d9ata/runs')\n",
    "model_names = sorted(name for name in models.__dict__\n",
    "    if name.islower() and not name.startswith(\"__\")\n",
    "    and callable(models.__dict__[name]))\n",
    "optimizer_names = [\"sgd\", \"adam\", \"rmsprop\"]\n",
    "\n",
    "def parse_args():\n",
    "    '''Parse input arguments.'''\n",
    "    parser = argparse.ArgumentParser(description='Digit Recognition')\n",
    "    parser.add_argument('--dataset-root', default='./data',\n",
    "                        help='train dataset path')\n",
    "    parser.add_argument('--arch', default='mobilenetv2_cifar', choices=model_names,\n",
    "                        help='model architecture: {} (default: mobilenetv2_cifar)'.format(' | '.join(model_names)))\n",
    "    parser.add_argument('--gpu-id', type=int, default=-1,\n",
    "                        help='gpu called when train')\n",
    "    parser.add_argument('--alphabet', default='0123456789',\n",
    "                        help='label alphabet, string format or file')\n",
    "    parser.add_argument('--optimizer', default='rmsprop', choices=optimizer_names,\n",
    "                        help='optimizer options: {} (default: rmsprop)'.format(' | '.join(optimizer_names)))\n",
    "    parser.add_argument('--max-epoch', type=int, default='30',\n",
    "                        help='number of total epochs to run (default: 30)')\n",
    "    parser.add_argument('--not-pretrained', dest='pretrained', action='store_false',\n",
    "                        help='initialize model with random weights (default: pretrained on cifar10)')\n",
    "    parser.add_argument('--validate-interval', type=int, default=1,\n",
    "                        help='Interval to be displayed')\n",
    "    parser.add_argument('--save-interval', type=int, default=1,\n",
    "                        help='save a model')\n",
    "    parser.add_argument('--workers', default=4, type=int,\n",
    "                        help='number of data loading workers (default: 4)')\n",
    "    parser.add_argument('--batch-size', type=int, default=64,\n",
    "                        help='batch size to train a model')\n",
    "    parser.add_argument('--train-samples', default=640000, type=int,\n",
    "                        help='train sample number')\n",
    "    parser.add_argument('--image-size', type=int, default=32,\n",
    "                        help='maximum size of longer image side used for training (default: 32)')\n",
    "    parser.add_argument('--lr', type=float, default=1e-3,\n",
    "                        help='initial learning rate (default: 1e-3)')\n",
    "    parser.add_argument('--decay-rate', type=float, default=0.1,\n",
    "                        help='learning rate decay')\n",
    "    parser.add_argument('--momentum', type=float, default=0.9,\n",
    "                        help='momentum')\n",
    "    parser.add_argument('--weight-decay', type=float, default=5e-4,\n",
    "                        help='weight decay (default: 5e-4)')\n",
    "    parser.add_argument('--print-freq', type=int, default=10,\n",
    "                        help='print frequency (default: 10)')\n",
    "    parser.add_argument('--directory', metavar='EXPORT_DIR', default='./checkpoint',\n",
    "                        help='Where to store samples and models')\n",
    "    parser.add_argument('--rnn', action='store_true',\n",
    "                        help='Train the model with model of rnn')\n",
    "    parser.add_argument('--resume', default='', type=str, metavar='FILENAME',\n",
    "                        help='name of the latest checkpoint (default: None)')\n",
    "    parser.add_argument('--test-only', action='store_true',\n",
    "                        help='test only')\n",
    "    args = parser.parse_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    for i, sample in enumerate(train_loader):\n",
    "        # Measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        # Zero out gradients so we can accumulate new ones over batches\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # step 2. Get our inputs targets ready for the network.\n",
    "        # targets is a list of `torch.IntTensor` with `batch_size` size.\n",
    "        target_lengths = sample.target_lengths.to(device)\n",
    "        targets = sample.targets # Expected targets to have CPU Backend\n",
    "\n",
    "        # step 3. Run out forward pass.\n",
    "        images = sample.images\n",
    "        if isinstance(images, tuple):\n",
    "            targets = targets.to(device)\n",
    "            log_probs = []\n",
    "            for image in images:\n",
    "                image = image.unsqueeze(0).to(device)\n",
    "                log_prob = model(image).squeeze(1)\n",
    "                log_probs.append(log_prob)\n",
    "            input_lengths = torch.IntTensor([i.size(0) for i in log_probs]).to(device)\n",
    "            log_probs = pad_sequence(log_probs)\n",
    "        else: # Batch\n",
    "            images = images.to(device)\n",
    "            log_probs = model(images)\n",
    "            #log_probs = pad_sequence(log_probs)\n",
    "            input_lengths = torch.full((images.size(0),), log_probs.size(0), dtype=torch.int32, device=device)\n",
    "\n",
    "        # step 4. Compute the loss, gradients, and update the parameters\n",
    "        # by calling optimizer.step()\n",
    "        loss = criterion(log_probs, targets, input_lengths, target_lengths)\n",
    "        losses.update(loss.item())\n",
    "        loss.backward()\n",
    "\n",
    "        # do one step for multiple batches\n",
    "        # accumulated gradients are used\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if (i+1) % args.print_freq == 0 or i == 0 or (i+1) == len(train_loader):\n",
    "            print('>> Train: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})'.format(\n",
    "                   epoch+1, i+1, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses))\n",
    "\n",
    "    return losses.avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(dev_loader, model, epoch, converter):\n",
    "    batch_time = AverageMeter()\n",
    "    accuracy = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    num_correct = 0\n",
    "    num_verified = 0\n",
    "    end = time.time()\n",
    "\n",
    "    #for i, (images, targets) in enumerate(dev_loader):\n",
    "    for i, sample in enumerate(dev_loader):\n",
    "        images = sample.images\n",
    "        targets = sample.targets\n",
    "        if isinstance(images, tuple):\n",
    "            preds = []\n",
    "            for image in images:\n",
    "                image = image.unsqueeze(0).to(device)\n",
    "                log_prob = model(image)\n",
    "                preds.append(converter.best_path_decode(log_prob, strings=False))\n",
    "        else: # Batch\n",
    "            images = images.to(device)\n",
    "            log_probs = model(images)\n",
    "            preds = converter.best_path_decode(log_probs, strings=False)\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        num_verified += len(targets)\n",
    "        for pred, target in zip(preds, targets):\n",
    "            print(pred)\n",
    "            print(target)\n",
    "            if pred == target:\n",
    "                num_correct += 1\n",
    "        accuracy.update(num_correct / num_verified)\n",
    "\n",
    "        if (i+1) % args.print_freq == 0 or i == 0 or (i+1) == len(dev_loader):\n",
    "            print('>> Val: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Accu {accuracy.val:.3f}'.format(\n",
    "                   epoch+1, i+1, len(dev_loader), batch_time=batch_time, accuracy=accuracy))\n",
    "\n",
    "    return accuracy.val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, directory):\n",
    "    filename = os.path.join(directory, '{}_epoch_{}.pth.tar'.format(state['arch'], state['epoch']))\n",
    "    with contextlib.suppress(FileNotFoundError):\n",
    "        os.remove(filename)\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        print('>>>> save best model at epoch: {}'.format(state['epoch']))\n",
    "        filename_best = os.path.join(directory, '{}_best.pth.tar'.format(state['arch']))\n",
    "        with contextlib.suppress(FileNotFoundError):\n",
    "            os.remove(filename_best)\n",
    "        shutil.copyfile(filename, filename_best)\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def set_batchnorm_eval(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('BatchNorm') != -1:\n",
    "        # freeze running mean and std:\n",
    "        # we do training one image at a time\n",
    "        # so the statistics would not be per batch\n",
    "        # hence we choose freezing (ie using imagenet statistics)\n",
    "        m.eval()\n",
    "        # # freeze parameters:\n",
    "        # # in fact no need to freeze scale and bias\n",
    "        # # they can be learned\n",
    "        # # that is why next two lines are commented\n",
    "        # for p in m.parameters():\n",
    "            # p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# alphabet/alphabet_decode_5990.txt\n",
    "sys.argv = ['main.py','--dataset-root','alphabet','--arch','densenet121','--alphabet','alphabet/alphabet_decode_5990.txt',\n",
    "            '--lr','5e-5','--max-epoch','100','--optimizer','rmsprop','--gpu-id','-1','--resume','densenet121_pretrained.pth.tar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Creating directory if it does not exist:\n",
      ">> './checkpoint/densenet121_rmsprop_lr5.0e-05_wd5.0e-04_bsize64_imsize32'\n",
      ">> Using pre-trained model 'densenet121'\n",
      "tensor([7.4218e-01, 8.8893e-01, 8.0820e-01, 2.7212e-01, 1.4403e-01, 9.5373e-01,\n",
      "        6.4722e-01, 2.6375e-01, 7.0618e-01, 6.2723e-01, 2.5117e-04, 7.9310e-01,\n",
      "        9.3245e-01, 7.4960e-01, 3.1593e-01, 4.7778e-01, 2.7406e-01, 9.4548e-01,\n",
      "        5.9349e-01, 2.8697e-01, 1.8703e-01, 3.6511e-01, 2.3044e-01, 3.6623e-01,\n",
      "        5.0227e-01, 8.3012e-01, 6.0206e-01, 5.9548e-01, 7.1892e-01, 8.5128e-01,\n",
      "        1.9017e-01, 9.0602e-01, 4.4408e-01, 3.1608e-01, 7.3952e-01, 4.7534e-01,\n",
      "        9.3300e-01, 6.1546e-01, 9.0827e-01, 6.1992e-01, 4.7572e-01, 6.6155e-01,\n",
      "        9.6055e-01, 6.4383e-01, 1.2616e-01, 9.5934e-01, 4.4369e-01, 2.8594e-01,\n",
      "        4.8653e-01, 1.8289e-01, 6.8133e-01, 2.5010e-01, 2.5496e-01, 8.0735e-01,\n",
      "        6.3919e-01, 2.4404e-02, 2.1090e-01, 8.5038e-01, 4.8784e-01, 9.4624e-01,\n",
      "        2.2707e-01, 9.6654e-03, 4.7014e-01, 5.8544e-01, 3.5637e-02, 2.1762e-01,\n",
      "        6.2207e-01, 7.6741e-01, 5.4378e-01, 9.5596e-01, 5.9267e-01, 9.6673e-01,\n",
      "        9.7672e-01, 8.4396e-02, 5.4657e-01, 5.1516e-02, 1.0777e-01, 6.7646e-01,\n",
      "        3.4923e-01, 3.2348e-01, 9.7807e-01, 8.9007e-03, 3.4063e-01, 6.5259e-01,\n",
      "        4.3815e-04, 5.7109e-01, 2.0374e-01, 5.2469e-01, 9.1004e-01, 8.1360e-01,\n",
      "        7.8141e-02, 6.6952e-03, 9.6162e-01, 5.2716e-01, 7.7459e-01, 3.9158e-01,\n",
      "        3.2490e-01, 8.8340e-01, 4.5109e-01, 1.6642e-01, 6.0770e-01, 6.6632e-01,\n",
      "        8.4736e-01, 2.9337e-01, 1.0269e-02, 9.7871e-01, 2.6096e-01, 5.9297e-01,\n",
      "        2.7844e-01, 8.4369e-01, 7.6754e-01, 9.2357e-01, 2.3311e-01, 2.4151e-02,\n",
      "        5.2286e-01, 9.2050e-01, 9.9742e-01, 6.5380e-02, 8.6402e-01, 2.9667e-01,\n",
      "        2.6161e-01, 4.6816e-01, 5.6325e-01, 6.3453e-01, 8.7825e-01, 1.5451e-01,\n",
      "        9.8031e-01, 8.2947e-01, 6.9348e-01, 5.2961e-01, 7.2492e-01, 9.0764e-01,\n",
      "        5.0228e-01, 8.7127e-01, 9.4824e-01, 9.7531e-01, 2.5237e-01, 5.9836e-02,\n",
      "        3.3802e-01, 8.4759e-01, 6.9375e-01, 9.4670e-01, 1.6479e-01, 5.8952e-01,\n",
      "        5.5730e-01, 1.9068e-01, 9.6336e-02, 4.5135e-01, 1.9571e-01, 5.5512e-01,\n",
      "        2.1119e-01, 5.1678e-01, 5.6412e-01, 8.7415e-01, 4.2882e-01, 9.8617e-01,\n",
      "        3.9912e-01, 1.5426e-01, 9.9602e-01, 3.4962e-03, 6.5781e-01, 8.4256e-01,\n",
      "        2.9194e-01, 9.6463e-01, 7.5400e-01, 8.2235e-01, 2.0344e-01, 6.3207e-01,\n",
      "        7.5438e-01, 3.0399e-02, 6.1110e-01, 3.4405e-02, 8.8973e-01, 8.5269e-01,\n",
      "        4.8009e-01, 6.1049e-01, 3.0750e-02, 4.3797e-04, 5.6673e-01, 3.0258e-01,\n",
      "        7.4763e-01, 5.9562e-01, 7.5036e-01, 8.6370e-01, 3.4468e-01, 3.1638e-02,\n",
      "        9.1706e-01, 8.9594e-01, 4.3122e-01, 9.3971e-01, 1.8344e-01, 5.5587e-01,\n",
      "        6.4607e-01, 9.2214e-01, 4.4109e-02, 7.5364e-01, 2.7606e-01, 6.8041e-01,\n",
      "        7.0201e-01, 5.9622e-02, 8.9732e-01, 3.4118e-01, 2.8815e-01, 3.3016e-01,\n",
      "        5.3011e-01, 3.8324e-01, 8.8054e-01, 6.1077e-01, 5.2206e-01, 8.6321e-01,\n",
      "        7.3447e-02, 4.8276e-01, 3.2370e-01, 3.3593e-01, 1.3025e-01, 5.8739e-01,\n",
      "        4.5112e-01, 3.3269e-01, 1.0056e-01, 2.8880e-01, 9.6685e-02, 2.7745e-01,\n",
      "        5.7788e-01, 1.0149e-01, 7.0468e-01, 5.0110e-04, 8.4610e-01, 3.4737e-01,\n",
      "        9.7545e-01, 7.1672e-01, 4.5282e-01, 5.2677e-01, 3.8433e-01, 9.8044e-01,\n",
      "        4.7486e-01, 7.7017e-01, 3.1206e-01, 6.6092e-01, 2.9212e-01, 7.8410e-01,\n",
      "        4.7767e-02, 2.1932e-02, 3.5562e-01, 3.2960e-02, 9.6480e-01, 6.0743e-01,\n",
      "        9.4438e-02, 8.5011e-01, 5.5182e-01, 1.0452e-01, 4.0771e-01, 7.6853e-01,\n",
      "        8.1441e-01, 8.1221e-01, 7.4290e-02, 8.0490e-01, 3.6654e-01, 9.9867e-01,\n",
      "        5.6837e-01, 1.7859e-02, 8.3432e-01, 1.2523e-01, 1.9574e-01, 6.3292e-01,\n",
      "        1.0639e-01, 6.2501e-01, 2.0920e-01, 3.3230e-01, 3.8140e-01, 9.3237e-01,\n",
      "        1.3418e-01, 3.2163e-01, 8.4611e-01, 8.9717e-02, 4.0140e-02, 2.6158e-01,\n",
      "        7.1760e-01, 6.1734e-01, 8.2240e-01, 2.3373e-01, 2.7749e-01, 2.2120e-02,\n",
      "        2.6973e-01, 1.1949e-01, 8.3988e-01, 4.5476e-01, 2.2500e-01, 3.3732e-01,\n",
      "        1.3665e-01, 1.2367e-01, 4.5107e-01, 6.0804e-01, 6.9929e-01, 4.7754e-01,\n",
      "        9.7584e-01, 8.3986e-01, 3.6689e-02, 8.5272e-01, 7.3263e-01, 4.5545e-01,\n",
      "        3.1130e-01, 7.3159e-02, 8.2082e-01, 4.6847e-02, 3.8248e-01, 9.9378e-01,\n",
      "        7.1692e-02, 2.1329e-01, 5.7341e-01, 7.6505e-01, 1.6123e-01, 9.6174e-01,\n",
      "        3.2420e-01, 6.4479e-01, 8.0008e-01, 7.6447e-01, 3.5998e-01, 1.9670e-01,\n",
      "        2.1575e-01, 9.5327e-01, 1.0298e-01, 4.5846e-01, 4.2160e-01, 9.2514e-01,\n",
      "        8.2646e-01, 9.2872e-02, 5.1211e-01, 9.2413e-01, 3.4729e-01, 1.5624e-01,\n",
      "        4.1903e-01, 2.5063e-01, 4.4608e-01, 8.1358e-01, 5.6247e-01, 9.0273e-01,\n",
      "        8.7015e-01, 8.1014e-01, 3.2038e-01, 1.7053e-01, 9.7601e-01, 6.1828e-01,\n",
      "        6.5018e-01, 9.7714e-01, 3.7830e-01, 5.9661e-02, 4.9008e-01, 1.6435e-01,\n",
      "        9.4011e-01, 4.1614e-01, 2.4950e-01, 8.1519e-02, 8.1669e-01, 6.4933e-01,\n",
      "        2.3374e-01, 3.9048e-01, 7.6967e-01, 1.5215e-01, 3.0051e-01, 7.2174e-01,\n",
      "        6.0387e-01, 1.8227e-01, 3.7270e-01, 8.0289e-01, 2.1839e-01, 2.1378e-01,\n",
      "        1.5026e-01, 2.7423e-02, 3.2012e-01, 4.1973e-01, 7.4267e-01, 9.7685e-01,\n",
      "        8.2895e-01, 4.7701e-01, 6.3198e-01, 2.3336e-01, 8.9807e-01, 8.1339e-01,\n",
      "        8.0518e-01, 2.3950e-01, 3.9222e-01, 2.0534e-01, 3.0658e-01, 9.2858e-01])\n"
     ]
    }
   ],
   "source": [
    "global args, device\n",
    "\n",
    "args = parse_args()\n",
    "\n",
    "if args.gpu_id < 0:\n",
    "    device = torch.device(\"cpu\")\n",
    "else:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu_id)\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# create export dir if it doesnt exist\n",
    "directory = \"{}\".format(args.arch)\n",
    "directory += \"_{}_lr{:.1e}_wd{:.1e}\".format(args.optimizer, args.lr, args.weight_decay)\n",
    "directory += \"_bsize{}_imsize{}\".format(args.batch_size, args.image_size)\n",
    "\n",
    "args.directory = os.path.join(args.directory, directory)\n",
    "print(\">> Creating directory if it does not exist:\\n>> '{}'\".format(args.directory))\n",
    "if not os.path.exists(args.directory):\n",
    "    os.makedirs(args.directory)\n",
    "\n",
    "# initialize model\n",
    "if args.pretrained:\n",
    "    print(\">> Using pre-trained model '{}'\".format(args.arch))\n",
    "else:\n",
    "    print(\">> Using model from scratch (random weights) '{}'\".format(args.arch))\n",
    "\n",
    "# load alphabet from file\n",
    "if os.path.isfile(args.alphabet):\n",
    "    alphabet = ''\n",
    "    with open(args.alphabet, mode='r', encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            alphabet += line.strip()\n",
    "    args.alphabet = alphabet\n",
    "\n",
    "model_params = {}\n",
    "model_params['architecture'] = args.arch\n",
    "model_params['num_classes'] = len(args.alphabet) + 1\n",
    "model_params['mean'] = (0.5,)\n",
    "model_params['std'] = (0.5,)\n",
    "model_params['pretrained'] = args.pretrained\n",
    "model = init_network(model_params)\n",
    "model = model.to(device)\n",
    "\n",
    "model_path = 'pretrained/densenet121_pretrained.pth'\n",
    "checkpoint = torch.load(model_path,map_location = 'cpu')\n",
    "\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "# for i, j in model.named_parameters():\n",
    "#     print(i)\n",
    "#     print(j)\n",
    "idx = random.sample(range(384),384)\n",
    "random_param = torch.rand(384)\n",
    "zero_param = torch.Tensor([0.0]*384)\n",
    "\n",
    "#checkpoint[\"classifier.weight\"] = torch.rand(5990,384)\n",
    "#checkpoint[\"features.8.weight\"] = zero_param\n",
    "checkpoint[\"features.8.weight\"] = random_param\n",
    "#checkpoint[\"features.8.weight\"] = checkpoint[\"features.8.weight\"][idx]\n",
    "#print(checkpoint[\"classifier.weight\"])\n",
    "print(checkpoint[\"features.8.weight\"])\n",
    "\n",
    "torch.save(checkpoint, \"pretrained/weights.pth\")\n",
    "\n",
    "# model = DenseNet(img_height=32, drop_rate=0.2, num_classes=len(args.alphabet) + 1)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 280)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.dataset import InMemoryDigitsDataset, DigitsDataset, collate_train, collate_dev, inmemory_train, inmemory_dev\n",
    "\n",
    "num = 1000\n",
    "dev_num = int(num/5)\n",
    "use_file = 1\n",
    "text_length = 10\n",
    "font_size = 0\n",
    "font_id = 1\n",
    "space_width = 1\n",
    "text_color = '#282828'\n",
    "thread_count = 8\n",
    "\n",
    "random_skew = False\n",
    "skew_angle = 0\n",
    "random_blur = False\n",
    "blur = 0\n",
    "\n",
    "distorsion = 0\n",
    "background = 1\n",
    "\n",
    "# for i in range(100):\n",
    "#     img, label = train_dataset.__getitem__(i)\n",
    "#     print(img.shape,label)\n",
    "# plt.imshow(train_dataset.__getitem__(0)[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217998\n",
      "63161\n",
      "49316\n",
      "28221\n",
      ">> Train: [1][1/16]\tTime 11.922 (11.922)\tData 0.116 (0.116)\tLoss 0.5226 (0.5226)\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CTCLoss()\n",
    "# criterion = nn.CTCLoss(zero_infinity=True)\n",
    "criterion = criterion.to(device)\n",
    "# define optimizer\n",
    "if args.optimizer == 'sgd':\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "elif args.optimizer == 'rmsprop':\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "elif args.optimizer == 'adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "converter = LabelConverter(args.alphabet, ignore_case=False)\n",
    "\n",
    "# define learning rate decay schedule\n",
    "# TODO: maybe pass as argument in future implementation?\n",
    "exp_decay = math.exp(-0.1)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=exp_decay)\n",
    "# step_decay = 1\n",
    "# gamma_decay = 0.5\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_decay, gamma=gamma_decay)\n",
    "\n",
    "is_best = False\n",
    "best_accuracy = 0.0\n",
    "accuracy = 0.0\n",
    "start_epoch = 0\n",
    "\n",
    "for epoch in range(start_epoch, args.max_epoch):\n",
    "    text_meta, text_img = gen_text_img(num, use_file, text_length, font_size, font_id, space_width, background, text_color,\n",
    "                          blur, random_blur, distorsion, skew_angle, random_skew, thread_count)\n",
    "    dev_meta, dev_img = gen_text_img(dev_num, use_file, text_length, font_size, font_id, space_width, background, text_color,\n",
    "                          blur, random_blur, distorsion, skew_angle, random_skew, thread_count)\n",
    "\n",
    "    index_converter = IndexConverter(args.alphabet, ignore_case=False)\n",
    "\n",
    "    train_dataset = InMemoryDigitsDataset(mode='train',text=text_meta,img=text_img,total=num,\n",
    "                                      transform=transform, converter = index_converter)\n",
    "    dev_dataset = InMemoryDigitsDataset(mode='dev', text=dev_meta, img=dev_img, total=dev_num,\n",
    "                                    transform=transform, converter = index_converter)\n",
    "\n",
    "    train_loader = data.DataLoader(train_dataset, batch_size=args.batch_size, collate_fn=collate_train,\n",
    "                               shuffle=True, num_workers=args.workers, pin_memory=True)\n",
    "    dev_loader = data.DataLoader(dev_dataset, batch_size=args.batch_size, collate_fn=collate_dev,\n",
    "                             shuffle=False, num_workers=args.workers, pin_memory=True)\n",
    "\n",
    "    # aujust learning rate for each epoch\n",
    "    scheduler.step()\n",
    "\n",
    "    # train for one epoch on train set\n",
    "    loss = train(train_loader, model, criterion, optimizer, epoch)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    if (epoch + 1) % args.validate_interval == 0:\n",
    "        with torch.no_grad():\n",
    "            accuracy = validate(dev_loader, model, epoch, converter)\n",
    "\n",
    "    # # evaluate on test datasets every test_freq epochs\n",
    "    # if (epoch + 1) % args.test_freq == 0:\n",
    "    #     with torch.no_grad():\n",
    "    #         test(args.test_datasets, model)\n",
    "\n",
    "    # remember best accuracy and save checkpoint\n",
    "    is_best = accuracy > 0.0 and accuracy >= best_accuracy\n",
    "    best_accuracy = max(accuracy, best_accuracy)\n",
    "\n",
    "    if (epoch + 1) % args.save_interval == 0:\n",
    "        save_checkpoint({\n",
    "            'arch': args.arch,\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_accuracy': best_accuracy,\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "        }, is_best, args.directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
