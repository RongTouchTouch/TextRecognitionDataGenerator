{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import math\n",
    "import shutil\n",
    "import contextlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from utils.converter import LabelConverter, IndexConverter\n",
    "from datasets.dataset import InMemoryDigitsDataset, DigitsDataset, collate_train, collate_dev, inmemory_train, inmemory_dev\n",
    "from generate import gen_text_img\n",
    "\n",
    "import models\n",
    "from models.crnn import init_network\n",
    "from models.densenet_ import DenseNet\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"always\")\n",
    "\n",
    "\n",
    "# from tensorboardX import SummaryWriter\n",
    "# writer = SummaryWriter('./d9ata/runs')\n",
    "model_names = sorted(name for name in models.__dict__\n",
    "    if name.islower() and not name.startswith(\"__\")\n",
    "    and callable(models.__dict__[name]))\n",
    "optimizer_names = [\"sgd\", \"adam\", \"rmsprop\"]\n",
    "\n",
    "def parse_args():\n",
    "    '''Parse input arguments.'''\n",
    "    parser = argparse.ArgumentParser(description='Digit Recognition')\n",
    "    parser.add_argument('--dataset-root', default='./data',\n",
    "                        help='train dataset path')\n",
    "    parser.add_argument('--arch', default='mobilenetv2_cifar', choices=model_names,\n",
    "                        help='model architecture: {} (default: mobilenetv2_cifar)'.format(' | '.join(model_names)))\n",
    "    parser.add_argument('--gpu-id', type=int, default=-1,\n",
    "                        help='gpu called when train')\n",
    "    parser.add_argument('--alphabet', default='0123456789',\n",
    "                        help='label alphabet, string format or file')\n",
    "    parser.add_argument('--optimizer', default='rmsprop', choices=optimizer_names,\n",
    "                        help='optimizer options: {} (default: rmsprop)'.format(' | '.join(optimizer_names)))\n",
    "    parser.add_argument('--max-epoch', type=int, default='30',\n",
    "                        help='number of total epochs to run (default: 30)')\n",
    "    parser.add_argument('--not-pretrained', dest='pretrained', action='store_false',\n",
    "                        help='initialize model with random weights (default: pretrained on cifar10)')\n",
    "    parser.add_argument('--validate-interval', type=int, default=1,\n",
    "                        help='Interval to be displayed')\n",
    "    parser.add_argument('--save-interval', type=int, default=1,\n",
    "                        help='save a model')\n",
    "    parser.add_argument('--workers', default=4, type=int,\n",
    "                        help='number of data loading workers (default: 4)')\n",
    "    parser.add_argument('--batch-size', type=int, default=64,\n",
    "                        help='batch size to train a model')\n",
    "    parser.add_argument('--train-samples', default=640000, type=int,\n",
    "                        help='train sample number')\n",
    "    parser.add_argument('--image-size', type=int, default=32,\n",
    "                        help='maximum size of longer image side used for training (default: 32)')\n",
    "    parser.add_argument('--lr', type=float, default=1e-3,\n",
    "                        help='initial learning rate (default: 1e-3)')\n",
    "    parser.add_argument('--decay-rate', type=float, default=0.1,\n",
    "                        help='learning rate decay')\n",
    "    parser.add_argument('--momentum', type=float, default=0.9,\n",
    "                        help='momentum')\n",
    "    parser.add_argument('--weight-decay', type=float, default=5e-4,\n",
    "                        help='weight decay (default: 5e-4)')\n",
    "    parser.add_argument('--print-freq', type=int, default=10,\n",
    "                        help='print frequency (default: 10)')\n",
    "    parser.add_argument('--directory', metavar='EXPORT_DIR', default='./checkpoint',\n",
    "                        help='Where to store samples and models')\n",
    "    parser.add_argument('--rnn', action='store_true',\n",
    "                        help='Train the model with model of rnn')\n",
    "    parser.add_argument('--resume', default='', type=str, metavar='FILENAME',\n",
    "                        help='name of the latest checkpoint (default: None)')\n",
    "    parser.add_argument('--test-only', action='store_true',\n",
    "                        help='test only')\n",
    "    args = parser.parse_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    for i, sample in enumerate(train_loader):\n",
    "        # Measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        # Zero out gradients so we can accumulate new ones over batches\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # step 2. Get our inputs targets ready for the network.\n",
    "        # targets is a list of `torch.IntTensor` with `batch_size` size.\n",
    "        target_lengths = sample.target_lengths.to(device)\n",
    "        targets = sample.targets # Expected targets to have CPU Backend\n",
    "\n",
    "        # step 3. Run out forward pass.\n",
    "        images = sample.images\n",
    "        if isinstance(images, tuple):\n",
    "            targets = targets.to(device)\n",
    "            log_probs = []\n",
    "            for image in images:\n",
    "                image = image.unsqueeze(0).to(device)\n",
    "                log_prob = model(image).squeeze(1)\n",
    "                log_probs.append(log_prob)\n",
    "            input_lengths = torch.IntTensor([i.size(0) for i in log_probs]).to(device)\n",
    "            log_probs = pad_sequence(log_probs)\n",
    "        else: # Batch\n",
    "            images = images.to(device)\n",
    "            log_probs = model(images)\n",
    "            #log_probs = pad_sequence(log_probs)\n",
    "            input_lengths = torch.full((images.size(0),), log_probs.size(0), dtype=torch.int32, device=device)\n",
    "\n",
    "        # step 4. Compute the loss, gradients, and update the parameters\n",
    "        # by calling optimizer.step()\n",
    "        loss = criterion(log_probs, targets, input_lengths, target_lengths)\n",
    "        losses.update(loss.item())\n",
    "        loss.backward()\n",
    "\n",
    "        # do one step for multiple batches\n",
    "        # accumulated gradients are used\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if (i+1) % args.print_freq == 0 or i == 0 or (i+1) == len(train_loader):\n",
    "            print('>> Train: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})'.format(\n",
    "                   epoch+1, i+1, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses))\n",
    "\n",
    "    return losses.avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(dev_loader, model, epoch, converter):\n",
    "    batch_time = AverageMeter()\n",
    "    accuracy = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    num_correct = 0\n",
    "    num_verified = 0\n",
    "    end = time.time()\n",
    "\n",
    "    #for i, (images, targets) in enumerate(dev_loader):\n",
    "    for i, sample in enumerate(dev_loader):\n",
    "        images = sample.images\n",
    "        targets = sample.targets\n",
    "        if isinstance(images, tuple):\n",
    "            preds = []\n",
    "            for image in images:\n",
    "                image = image.unsqueeze(0).to(device)\n",
    "                log_prob = model(image)\n",
    "                preds.append(converter.best_path_decode(log_prob, strings=False))\n",
    "        else: # Batch\n",
    "            images = images.to(device)\n",
    "            log_probs = model(images)\n",
    "            preds = converter.best_path_decode(log_probs, strings=False)\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        num_verified += len(targets)\n",
    "        for pred, target in zip(preds, targets):\n",
    "            print(pred)\n",
    "            print(target)\n",
    "            if pred == target:\n",
    "                num_correct += 1\n",
    "        accuracy.update(num_correct / num_verified)\n",
    "\n",
    "        if (i+1) % args.print_freq == 0 or i == 0 or (i+1) == len(dev_loader):\n",
    "            print('>> Val: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Accu {accuracy.val:.3f}'.format(\n",
    "                   epoch+1, i+1, len(dev_loader), batch_time=batch_time, accuracy=accuracy))\n",
    "\n",
    "    return accuracy.val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, directory):\n",
    "    filename = os.path.join(directory, '{}_epoch_{}.pth.tar'.format(state['arch'], state['epoch']))\n",
    "    with contextlib.suppress(FileNotFoundError):\n",
    "        os.remove(filename)\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        print('>>>> save best model at epoch: {}'.format(state['epoch']))\n",
    "        filename_best = os.path.join(directory, '{}_best.pth.tar'.format(state['arch']))\n",
    "        with contextlib.suppress(FileNotFoundError):\n",
    "            os.remove(filename_best)\n",
    "        shutil.copyfile(filename, filename_best)\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def set_batchnorm_eval(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('BatchNorm') != -1:\n",
    "        # freeze running mean and std:\n",
    "        # we do training one image at a time\n",
    "        # so the statistics would not be per batch\n",
    "        # hence we choose freezing (ie using imagenet statistics)\n",
    "        m.eval()\n",
    "        # # freeze parameters:\n",
    "        # # in fact no need to freeze scale and bias\n",
    "        # # they can be learned\n",
    "        # # that is why next two lines are commented\n",
    "        # for p in m.parameters():\n",
    "            # p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# alphabet/alphabet_decode_5990.txt\n",
    "sys.argv = ['main.py','--dataset-root','alphabet','--arch','densenet121','--alphabet','alphabet/alphabet_decode_5990.txt',\n",
    "            '--lr','5e-5','--max-epoch','100','--optimizer','rmsprop','--gpu-id','-1','--resume','densenet121_pretrained.pth.tar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Creating directory if it does not exist:\n",
      ">> './checkpoint/densenet121_rmsprop_lr5.0e-05_wd5.0e-04_bsize64_imsize32'\n",
      ">> Using pre-trained model 'densenet121'\n"
     ]
    }
   ],
   "source": [
    "global args, device\n",
    "\n",
    "args = parse_args()\n",
    "\n",
    "if args.gpu_id < 0:\n",
    "    device = torch.device(\"cpu\")\n",
    "else:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu_id)\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# create export dir if it doesnt exist\n",
    "directory = \"{}\".format(args.arch)\n",
    "directory += \"_{}_lr{:.1e}_wd{:.1e}\".format(args.optimizer, args.lr, args.weight_decay)\n",
    "directory += \"_bsize{}_imsize{}\".format(args.batch_size, args.image_size)\n",
    "\n",
    "args.directory = os.path.join(args.directory, directory)\n",
    "print(\">> Creating directory if it does not exist:\\n>> '{}'\".format(args.directory))\n",
    "if not os.path.exists(args.directory):\n",
    "    os.makedirs(args.directory)\n",
    "\n",
    "# initialize model\n",
    "if args.pretrained:\n",
    "    print(\">> Using pre-trained model '{}'\".format(args.arch))\n",
    "else:\n",
    "    print(\">> Using model from scratch (random weights) '{}'\".format(args.arch))\n",
    "\n",
    "# load alphabet from file\n",
    "if os.path.isfile(args.alphabet):\n",
    "    alphabet = ''\n",
    "    with open(args.alphabet, mode='r', encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            alphabet += line.strip()\n",
    "    args.alphabet = alphabet\n",
    "\n",
    "model_params = {}\n",
    "model_params['architecture'] = args.arch\n",
    "model_params['num_classes'] = len(args.alphabet) + 1\n",
    "model_params['mean'] = (0.5,)\n",
    "model_params['std'] = (0.5,)\n",
    "model_params['pretrained'] = args.pretrained\n",
    "model = init_network(model_params)\n",
    "model = model.to(device)\n",
    "\n",
    "model_path = 'pretrained/densenet121_pretrained.pth'\n",
    "checkpoint = torch.load(model_path,map_location = 'cpu')\n",
    "model.load_state_dict(checkpoint)\n",
    "# model = DenseNet(img_height=32, drop_rate=0.2, num_classes=len(args.alphabet) + 1)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 280)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.dataset import InMemoryDigitsDataset, DigitsDataset, collate_train, collate_dev, inmemory_train, inmemory_dev\n",
    "\n",
    "num = 1000\n",
    "dev_num = int(num/5)\n",
    "use_file = 1\n",
    "text_length = 10\n",
    "font_size = 0\n",
    "font_id = 1\n",
    "space_width = 1\n",
    "text_color = '#282828'\n",
    "thread_count = 8\n",
    "\n",
    "random_skew = False\n",
    "skew_angle = 0\n",
    "random_blur = False\n",
    "blur = 0\n",
    "\n",
    "distorsion = 0\n",
    "background = 1\n",
    "\n",
    "\n",
    "\n",
    "# for i in range(100):\n",
    "#     img, label = train_dataset.__getitem__(i)\n",
    "#     print(img.shape,label)\n",
    "# plt.imshow(train_dataset.__getitem__(0)[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138868\n",
      "156987\n",
      "43185\n",
      "60313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 234, in _feed\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/.local/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 295, in reduce_storage\n",
      "    fd, size = storage._share_fd_()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 234, in _feed\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "RuntimeError: unable to write to file </torch_32657_3948723417>\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/.local/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 295, in reduce_storage\n",
      "    fd, size = storage._share_fd_()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 234, in _feed\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "RuntimeError: unable to write to file </torch_32658_2956650664>\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "  File \"/home/jovyan/.local/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 295, in reduce_storage\n",
      "    fd, size = storage._share_fd_()\n",
      "RuntimeError: unable to write to file </torch_32656_1800311011>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 234, in _feed\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "  File \"/home/jovyan/.local/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 295, in reduce_storage\n",
      "    fd, size = storage._share_fd_()\n",
      "RuntimeError: unable to write to file </torch_32656_3527042512>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid 32659) is killed by signal: Bus error. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-78a3f394ae57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# train for one epoch on train set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# evaluate on validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-e3dce5081f6b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, criterion, optimizer, epoch)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0;31m#log_probs = pad_sequence(log_probs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0minput_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TextRecognitionDataGenerator/models/crnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# x -> features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;31m# features -> pool -> flatten -> classifier -> softmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TextRecognitionDataGenerator/models/densenet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, init_features)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minit_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_children\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mnew_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TextRecognitionDataGenerator/models/densenet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *prev_features)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mbottleneck_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbn_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mprev_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mbottleneck_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbn_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mprev_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mnew_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbottleneck_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_rate\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TextRecognitionDataGenerator/models/densenet.py\u001b[0m in \u001b[0;36mbn_function\u001b[0;34m(*inputs)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbn_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mconcated_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mbottleneck_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcated_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbottleneck_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1621\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   1622\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1623\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1624\u001b[0m     )\n\u001b[1;32m   1625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36mhandler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;31m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;31m# Python can still get and update the process status successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0m_error_if_any_worker_fails\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprevious_handler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0mprevious_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 32659) is killed by signal: Bus error. "
     ]
    }
   ],
   "source": [
    "criterion = nn.CTCLoss()\n",
    "# criterion = nn.CTCLoss(zero_infinity=True)\n",
    "criterion = criterion.to(device)\n",
    "# define optimizer\n",
    "if args.optimizer == 'sgd':\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "elif args.optimizer == 'rmsprop':\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "elif args.optimizer == 'adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "converter = LabelConverter(args.alphabet, ignore_case=False)\n",
    "\n",
    "# define learning rate decay schedule\n",
    "# TODO: maybe pass as argument in future implementation?\n",
    "exp_decay = math.exp(-0.1)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=exp_decay)\n",
    "# step_decay = 1\n",
    "# gamma_decay = 0.5\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_decay, gamma=gamma_decay)\n",
    "\n",
    "is_best = False\n",
    "best_accuracy = 0.0\n",
    "accuracy = 0.0\n",
    "start_epoch = 0\n",
    "\n",
    "for epoch in range(start_epoch, args.max_epoch):\n",
    "    text_meta, text_img = gen_text_img(num, use_file, text_length, font_size, font_id, space_width, background, text_color,\n",
    "                          blur, random_blur, distorsion, skew_angle, random_skew, thread_count)\n",
    "    dev_meta, dev_img = gen_text_img(dev_num, use_file, text_length, font_size, font_id, space_width, background, text_color,\n",
    "                          blur, random_blur, distorsion, skew_angle, random_skew, thread_count)\n",
    "\n",
    "    index_converter = IndexConverter(args.alphabet, ignore_case=False)\n",
    "\n",
    "    train_dataset = InMemoryDigitsDataset(mode='train',text=text_meta,img=text_img,total=num,\n",
    "                                      transform=transform, converter = index_converter)\n",
    "    dev_dataset = InMemoryDigitsDataset(mode='dev', text=dev_meta, img=dev_img, total=dev_num,\n",
    "                                    transform=transform, converter = index_converter)\n",
    "\n",
    "    train_loader = data.DataLoader(train_dataset, batch_size=args.batch_size, collate_fn=collate_train,\n",
    "                               shuffle=True, num_workers=args.workers, pin_memory=True)\n",
    "    dev_loader = data.DataLoader(dev_dataset, batch_size=args.batch_size, collate_fn=collate_dev,\n",
    "                             shuffle=False, num_workers=args.workers, pin_memory=True)\n",
    "    # aujust learning rate for each epoch\n",
    "    scheduler.step()\n",
    "\n",
    "    # train for one epoch on train set\n",
    "    loss = train(train_loader, model, criterion, optimizer, epoch)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    if (epoch + 1) % args.validate_interval == 0:\n",
    "        with torch.no_grad():\n",
    "            accuracy = validate(dev_loader, model, epoch, converter)\n",
    "\n",
    "    # # evaluate on test datasets every test_freq epochs\n",
    "    # if (epoch + 1) % args.test_freq == 0:\n",
    "    #     with torch.no_grad():\n",
    "    #         test(args.test_datasets, model)\n",
    "\n",
    "    # remember best accuracy and save checkpoint\n",
    "    is_best = accuracy > 0.0 and accuracy >= best_accuracy\n",
    "    best_accuracy = max(accuracy, best_accuracy)\n",
    "\n",
    "    if (epoch + 1) % args.save_interval == 0:\n",
    "        save_checkpoint({\n",
    "            'arch': args.arch,\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_accuracy': best_accuracy,\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "        }, is_best, args.directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAIABJREFUeJzt3X2clXP+x/HXp5TcVGwNS5MKkSilkQiVm5V1W7JY9QtLJERb1s2667eW3ULu1mqt5KZIKxvblt223P2wTRrZSimi0a5aEUll6vP743tNnaapc+bmmmvmzPv5eMzDOde5znV9rtGcz/neXJ+vuTsiIiI7UifpAEREpPpTshARkbSULEREJC0lCxERSUvJQkRE0lKyEBGRtJQsREQkLSULERFJS8lCRETS2inpACpL06ZNvWXLlkmHISJSo8yePfu/7p6Tbr+sSRYtW7YkPz8/6TBERGoUM/skk/3UDSUiImkpWYiISFpKFiIikpaShYiIpKVkISIiaSlZiIhIWkoWIiKSVtbcZyHZ5f33YfZs+MlPYNddk45GAD79FCZOhGbNoFMnOOAAMEs6qtrru+/gvfcgPx/q1YPLL4/3fEoWUu24w4UXhoTx85/DlVfCoEHwwx8mHVntNHs23HMPTJgAGzdu2d64MRxxBOTlheShBBKftWtDYpg9e8vP/Plb/n8cfXT8ycLcPd4zVJG8vDzXHdzZ4ZVX4JRTYNgwWLQIJk8O35z69oUhQ+DQQ5OOMPtt2gR/+UtIEq++Cg0bwmWXwcCB8PXXW39ozZ0LGzaE9xUnkE6dtiQRJZCySZcYcnK2/v126gS5ueX/HZvZbHfPS7ufkoVUNyefHP44Pv4Y6tcPCWPUKHjiidD07tkztDhOPFEfQpXtu+/gySfhvvtg4UJo3hwGD4ZLLw2JoDQbNsC8eaE7JF0CKf6QUwIJqjoxlEbJQmqkggLo2BHuvht+8YutX/vvf+H3v4eHHoLPP4fDDw8tjfPPD0lFym/FCnj4Yfjd78LvuVOnkJD79AmturIqTiCzZ29JIrU9gZRMDPn5sGDB1okhNSnEkRhKo2QhNVLfvvDnP8OyZbDHHqXvs24djBsXukjmz4d994Wrrw59tnvuWbXx1nQffAD33htaE+vXwxlnhCRx/PGV/yGVmkCKPyx3lECKu7Dq1MA5m+kSw157bX2dVZUYSqNkITXOsmXQqhVcc034AEvHHaZNg5EjYfp02G03+NnP4Nprw3GkdO4wc2ZItn/5CzRoAP37w3XXwcEHV20sJRPI7NnhQ7YmJZDUxFDcitpRYsjLCzPKqksrSslCapyhQ8PYxEcfwX77le29BQUhwYwfHwZne/cO35C7dIkn1pro++/DjKZ77oE5c0K3x6BBYbZZTtrVDKpOugTSqFHps7CqIoEUJ4bU8Zn588O/Odg6MRTHV50SQ2mqRbIws57A/UBd4DF3v7vE6y2Ax4EcYBXQ190Lo9d+C5xGuHHwb8Bg30GwShY12+rVYTD1jDPgmWfKf5zPPoMHH4RHH4WvvoJjjglJ46yzoG7dyou3Jlm9GkaPhgcegMJCaNMmjPX07Qu77JJ0dJn5/vvSB9HXrw+vFyeQkrOwKpJAMk0MqUmruieG0mSaLHD3WH4ICWIJsD9QH3gPaFtin+eB/tHjE4CnosfHAG9Gx6gLvAV039H5OnXq5FJz/fa37uD+7ruVc7xvvnG//373Vq3CcQ84wP3BB93XrKmc49cES5e6X3ede8OG4XfQo4f7yy+7b9yYdGSVY8MG9zlz3B97zP2KK9yPPNJ9553DtYJ7o0bu3bu7//zn7uPGuS9atP1r//Zb9zffdH/gAff+/d0PO8y9Tp0tx9prL/cf/9j9llvcX3zRfdky902bqvRyYwPkewaf6bG1LMzsaOB2dz8len5jlJzuStlnHnCKuxeamQGr3b1R9N6HgGMBA14D+rn7gu2dTy2LmmvDBth///CN9+9/r9xjFxXBiy+Grpe33w4D4AMHwlVXwT77VO65qotZs8L1TpwYvuWed15oSRxxRNKRxa+4BVJyEL20FkizZlvGGlJbDHvvve3gc01sMWSqOrQs+hC6noqf9wMeKrHPOEL3EkBvwIEm0fORwFfAauDO7ZxjAJAP5O+3336VnXCliowdG769/fWv8Z7nzTfde/VyN3OvX9/9oovc586N95xVZePG8I33uOO2fKseNsz900+Tjix5qS2QgQPdO3fe0gLZe+/sbTFkigxbFnEmi3NLSRYPlthnX+AFYA5hbKMQaAwcCPwF2D36eQs4fkfnUzdUzbRpk3u7dqHZX1V/pB9+6D5okPuuu4a/gB/9yH3atJr5IfHtt+6PPOLeunW4lhYt3O+913316qQjq942bHD//POa+f+8smWaLOKcP1AINE95ngssT93B3Ze7e2937wjcHG1bDfQC3nb3Ne6+BvgroHktWeiVV0INqKFDq66Zf+CB4ca+ZcvgzjtDN8Upp4Sb/MaO3TLrpjr7/HO49dYwa2zgwHBPyrPPwuLFYQpso0ZJR1i91asXBqiztWspDnEmi1lAazNrZWb1gfOByak7mFlTMyuO4UbCzCiAT4FuZraTmdUDugHbHa+QmmvEiHBT3QUXVP25f/ADuOkmWLoUxowJQ5kXXQQtW8Jdd8GqVVUfUzrz54fSGy1awK9+BcceC6+9Bu+8E8YmdlJpUIlJbMnC3YuAq4BphA/6Ce4+z8yGm9mZ0W7dgYVmtgjYG7gz2j6RMJPqfcIsqvfc/aW4YpVkzJkTbqYbPDjZch077xySxNy54Sa/du1CEmnePNwZvmRJcrFBSGLTp8OPfxyKKI4bB5dcEu6+fvFFOO44fUOW+OmmPEnMhRfCSy+F7qDtFalLyty54Sa/cePCjKpevcL9GsccU3UxbNgAzz0X4igoCN0mV10Vup2aNq26OCS7ZTobqhrdNC+1yaefhg/Cyy6rfokCoH37UOV26VK44QaYMQO6dg3rBkycuPW6DpXtq6/gt78N04n/539C0njsMfjkE7jlFiUKSYaShSTi/vtD18m11yYdyY7tuy/8+teh9fPgg6E667nnQuvW4fmaNZV3rqVLw++jefNQcbdNG5gyBf71r1DzqkGDyjuXSFkpWUiV++qrUH7ivPPCB2NNsNtuoQto0SL405/CDX3XXBPiv/FGWL48/TG25513wvKxBxwQyoT36hXGc/7+dzj1VI1HSPWgZCFVbvTo8I186NCkIym7unVDkcI334S33oKTTgpdRi1bhsqtc+dmdpyNG7cMTnfpEqYQDxsWWhdPPgkdOsR5FSJlp2QhVWrDhtAFddJJNf8DsUsXeP55+PBDuOKK0OI4/PCw0t/UqWEWU0lr14YFhtq0CS2IwsJQaXfZsrDgU7NmVX8dIplQspAqNX586LKpia2K7dl//1DRddmycH/G/Pmh+6h9+3D/xvr18J//wC9/GbqtBg2CJk1CufAPPwxThxs2TPoqRHZMU2elyriHD1CzUMAtW/viN2wId1Pfc0/olsrJCWXCv/8ezj57yxTcbL1+qVkynTqr+z2lykybFmb2jB2b3R+U9euHKa/9+oWb6R59FH74w9CCOPDApKMTKR+1LKTKnHgiLFwYVsJL8o5tEdlCN+VJtfLuu/CPfyRf2kNEykfJQqrEyJFhEHfAgKQjEZHyULKQ2H3ySZj5M2BA9SztISLpKVlI7EaNCgPagwcnHYmIlJeShcTqyy/hD3+A88+vOaU9RGRbShYSq9Gj4dtvs+smPJHaSMlCYrN+fSjtcfLJoQyGiNRcsSYLM+tpZgvNbLGZ3VDK6y3MbLqZzTWzmWaWG23vYWYFKT/rzOzsOGOVyjd+PPz732pViGSD2G7KM7O6wCLgZKCQsCb3Be4+P2Wf54GX3X2smZ0AXOzu/Uoc5wfAYiDX3ddu73y6Ka96cQ/Lk9atG1Z5y+Y7tkVqsupwU15nYLG7f+TuG4BngbNK7NMWmB49nlHK6wB9gL/uKFFI9TN1KsybF1oVShQiNV+cyaIZsCzleWG0LdV7wDnR415AQzNrUmKf84HxsUQosRkxIpTbPv/8pCMRkcoQZ7Io7ftkyT6voUA3M5sDdAM+A4o2H8BsH6AdMK3UE5gNMLN8M8tfuXJl5UQtFTZ7dliz+tproV69pKMRkcoQZ7IoBFJn1ucCWy0+6e7L3b23u3cEbo62rU7Z5SfAJHf/vrQTuPtod89z97ycnJzKjV7KbeRIaNRIpT1EskmcyWIW0NrMWplZfUJ30uTUHcysqZkVx3Aj8HiJY1yAuqBqlKVLw+pxAwaEhCEi2SG2ZOHuRcBVhC6kBcAEd59nZsPN7Mxot+7AQjNbBOwN3Fn8fjNrSWiZvBpXjFL5VNpDJDtpPQupNF9+GUp69O4NTz6ZdDQikonqMHVWapnf/16lPUSylZKFVIr16+GBB+BHPwrrbItIdtEa3FIpxo2D//wHnnoq6UhEJA5qWUiFbdoUpssefnhYZ1tEso9aFlJhU6fC/PmhVaHSHiLZSS0LqbARIyA3F847L+lIRCQuShZSIfn5MHOmSnuIZDslC6mQ4tIel12WdCQiEiclCym34tIel1+u0h4i2U7JQsrtvvugTh2V9hCpDZQspFxWrYI//hF++tOwboWIZDclCykXlfYQqV2ULKTMikt7nHJKWGdbRLKfkoWU2TPPwOefw7BhSUciIlVFyULKpLi0R4cOcMIJSUcjIlVF5T6kTP76V1iwAJ5+WqU9RGqTWFsWZtbTzBaa2WIzu6GU11uY2XQzm2tmM80sN+W1/czsFTNbYGbzo5XzJGEjRoQFjn7yk6QjEZGqFFuyMLO6wMPAqUBb4AIza1tit5HAk+7eHhgO3JXy2pPACHc/BOgMrIgrVsnMrFnw6qsq7SFSG8XZsugMLHb3j9x9A/AscFaJfdoC06PHM4pfj5LKTu7+NwB3X+Pua2OMVTIwciQ0bqzSHiK1UZzJohmwLOV5YbQt1XvAOdHjXkBDM2sCHAR8ZWYvmNkcMxsRtVQkIR9/DBMnhtIeDRsmHY2IVLU4k0Vpw59e4vlQoJuZzQG6AZ8BRYSB9+Oi148E9gcu2uYEZgPMLN/M8leuXFmJoUtJ990HdevCNdckHYmIJCHOZFEINE95ngssT93B3Ze7e2937wjcHG1bHb13TtSFVQS8CBxR8gTuPtrd89w9LycnJ67rqPVU2kNE4kwWs4DWZtbKzOoD5wOTU3cws6ZmVhzDjcDjKe/d08yKM8AJwPwYY5UdeOQRWLtWpT1EarPYkkXUIrgKmAYsACa4+zwzG25mZ0a7dQcWmtkiYG/gzui9GwldUNPN7H1Cl9Yf4opVtm/dOnjwQejZEw47LOloRCQpsd6U5+5TgCkltt2a8ngiMHE77/0b0D7O+CS9p59WaQ8RUbkP2YFNm+Cee6BjR+jRI+loRCRJKvch2zVlCnzwAYwbp9IeIrWdWhayXSNGwH77QZ8+SUciIklTspBS/fOf8NprKu0hIoGShZSquLTHpZcmHYmIVAdKFrKNjz6CP/0JrrhCpT1EJFCykG2otIeIlKRkIVv54gt4/HG48ELYd9+koxGR6kLJQrai0h4iUholC9msuLTHqafCoYcmHY2IVCdKFrLZU0/BihUq7SEi21KyEGBLaY8jjoDu3ZOORkSqG5X7EAD+8hdYuBDGj1dpDxHZVkYtCzP7k5mdlrL2hGSZESOgRQuV9hCR0mX64f8I8FPgQzO728zaxBiTVLF33oHXX4frroOd1NYUkVJklCzc/e/ufiFhadOlwN/M7P/M7GIzU+WgGm7kSNhjD7jkkqQjEZHqKuNuJTNrAlwEXArMAe4nJI+/xRKZVIklS+CFF1TaQ0R2LNMxixeA14FdgTPc/Ux3f87drwZ238H7eprZQjNbbGY3lPJ6CzObbmZzzWymmeWmvLbRzAqin8kl3yuVQ6U9RCQTmfZQP+Tu/yjtBXfPK227mdUFHgZOBgqBWWY22d3np+w2EnjS3cea2QnAXUC/6LXv3L1DhvFJORSX9ujbF/bZJ+loRKQ6y7Qb6hAz26P4iZntaWZXpnlPZ2Cxu3/k7huAZ4GzSuzTFpgePZ5RyusSo9/9Dr77Dn7+86QjEZHqLtNkcZm7f1X8xN2/BC5L855mwLKU54XRtlTvAedEj3sBDaOxEYAGZpZvZm+b2dmlncDMBkT75K9cuTLDSxHYUtrjxz9WaQ8RSS/TZFHHbMutWlEXU/007ynt1i4v8Xwo0M3M5gDdgM+Aoui1/aIurp8Co8zsgG0O5j7a3fPcPS8nJyfDSxGAJ5+ElStV2kNEMpPpmMU0YIKZ/Z7wgX8FMDXNewqB5inPc4HlqTu4+3KgN4CZ7Q6c4+6rU17D3T8ys5lAR2BJhvHKDhSX9ujUCbp1SzoaEakJMk0WvwAuBwYSWgyvAI+lec8soLWZtSK0GM4ntBI2M7OmwCp33wTcCDwebd8TWOvu66N9ugK/zTBWSeOll2DRInj2WZX2EJHMZJQsog/zR6KfjLh7kZldRWiV1AUed/d5ZjYcyHf3yUB34C4zc+A1YFD09kOAR81sE6Gr7O4Ss6ikAkaOhJYt4Zxz0u4qIgJkmCzMrDVhWmtboEHxdnfff0fvc/cpwJQS225NeTwRmFjK+/4PaJdJbFI2b78Nb7wB99+v0h4ikrlMB7jHEFoVRUAP4EngqbiCkviotIeIlEemyWIXd58OmLt/4u63AyfEF5bEobi0x8CBsPt277sXEdlWph0R66Ly5B9G4xCfAXvFF5bE4d57oV49uPrqpCMRkZom05bFtYS6UNcAnYC+QP+4gpLK99//wpgxKu0hIuWTtmUR3YD3E3cfBqwBLo49Kql0Ku0hIhWRtmXh7huBTql3cEvN8t138NBDcNpp0LZt0tGISE2U6ZjFHODPZvY88G3xRnd/IZaopFKptIeIVFSmyeIHwBdsPQPKASWLaq64tEdeHhx/fNLRiEhNlekd3BqnqKEmT4YPP4TnnlNpDxEpv0zv4B7DthVjcXfd2lXNFZf26N076UhEpCbLtBvq5ZTHDQhrTyzfzr5STbz1Frz5JjzwgEp7iEjFZNoN9afU52Y2Hvh7LBFJpRk5EvbcEy5WJ6KIVFCmN+WV1BrYrzIDkcq1eDFMmgRXXqnSHiJScZmOWXzD1mMW/yGscSHVVHFpj6uuSjoSEckGmXZDNYw7EKk8K1eG0h79+sEPf5h0NCKSDTLqhjKzXmbWOOX5HmZ2dnxhSUX87newbp1Ke4hI5cl0zOK24rWxAdz9K+C2dG8ys55mttDMFpvZDaW83sLMppvZXDObaWa5JV5vZGafmdlDGcZZ6xWX9jj9dDjkkKSjEZFskWmyKG2/HXZhRQUIHwZOJaywd4GZlaxMNBJ40t3bA8MJq/Gl+l/g1QxjFGDs2FBhVqU9RKQyZZos8s3sXjM7wMz2N7P7gNlp3tMZWOzuH7n7BuBZ4KwS+7QFpkePZ6S+bmadgL2BVzKMsdbbuDEMbB95JBx3XNLRiEg2yTRZXA1sAJ4DJgDfAYPSvKcZsCzleWG0LdV7wDnR415AQzNrEi20dA+g78dlMHFiKO0xbJhKe4hI5cp0NtS3wDZjDmmU9nFVsmTIUOAhM7sIeI2wAl8RcCUwxd2X7agyupkNAAYA7Ldf7b7tY+1auP56aN9epT1EpPJlOhvqb2a2R8rzPc1sWpq3FQLNU57nUqJEiLsvd/fe7t4RuDnatho4GrjKzJYSxjX+x8zuLnkCdx/t7nnunpeTk5PJpWSt3/wGPv0UHnwQ6tZNOhoRyTaZVgxqGs2AAsDdvzSzdGtwzwJam1krQovhfOCnqTuYWVNglbtvAm4EHo+Of2HKPhcBee5e1pZNrfHxxyFZXHCBypCLSDwyHbPYZGab+3nMrCWlVKFN5e5FwFXANGABMMHd55nZcDM7M9qtO7DQzBYRBrPvLFP0AsCQIaFQ4IgRSUciItkq05bFzcAbZlY8jfV4orGCHXH3KcCUEttuTXk8EZiY5hhPAE9kGGet88or8OKL8OtfQ7OS0wdERCpJpgPcU80sj5AgCoA/E2ZESYI2bIBrroEDDwytCxGRuGRaSPBSYDBhkLoA6AK8xdbLrEoVe/BBWLgQXn4Zdt456WhEJJtlOmYxGDgS+MTdewAdgZWxRSVp/fvfcMcdcNpp4UdEJE6ZJot17r4OwMx2dvcPgIPjC0vSufFGWL8e7rsv6UhEpDbIdIC7MLrP4kXgb2b2JVpWNTFvvRVqQN1wA7RunXQ0IlIbmPsOZ8Bu+wazbkBjYGpU86layMvL8/z8/KTDiN3GjXDUUaEbauFCrYInIhVjZrPdPS/dfpm2LDZzd1WBTdDjj8Ps2fDMM0oUIlJ1yrsGtyTgyy/hpptCRdkLLkg6GhGpTZQsapDbboNVq8KUWVWVFZGqpGRRQ7z/flgu9Yor4PDDk45GRGobJYsawD3cqd24MQwfnnQ0IlIblXmAW6re88/DzJnwyCPQpEnS0YhIbaSWRTX37bfw859Dhw5w2WVJRyMitZVaFtXcXXdBYSGMH69FjUQkOWpZVGNLloQ1Ki68EI49NuloRKQ2U7KoxoYMgfr14be/TToSEantYk0WZtbTzBaa2WIz22ZZVDNrYWbTzWyumc00s9yU7bPNrMDM5pnZFXHGWR1NnQqTJ8Mtt8C++yYdjYjUdmWuDZXxgc3qAouAk4FCwprcF7j7/JR9ngdedvexZnYCcLG79zOz+lFs681sd+BfwDHuvt3ihdlUG2rDBmjXLkyZff99rVUhIvGJrTZUGXQGFrv7R1FAzwJnAfNT9mkLXBc9nkGoakuJAoU7U8u6y+6/HxYtgilTlChEpHqI80O4GbAs5XlhtC3Ve8A50eNeQEMzawJgZs3NbG50jN/sqFWRTZYvDzfenXEGnHpq0tGIiARxJovSqheV7PMaCnQzszlAN+AzoAjA3Ze5e3vgQKC/me29zQnMBphZvpnlr1yZHQv3/eIXoRtKixqJSHUSZ7IoBJqnPM+lxIJJ7r7c3Xu7e0fg5mjb6pL7APOA40qewN1Hu3ueu+fl5ORUdvxV7s034emnYehQOOCApKMREdkizmQxC2htZq2iAevzgcmpO5hZUzMrjuFG4PFoe66Z7RI93hPoCiyMMdbEbdwIV18NubmhDLmISHUS2wC3uxeZ2VXANKAu8Li7zzOz4UC+u08GugN3mZkDrwGDorcfAtwTbTdgpLu/H1es1cFjj8GcOfDss7DbbklHIyKytdimzla1mjx1dtUqOOggOOwwmDFDa1WISNXJdOpsrZqSWl3demtYBe+BB5QoRKR6UrJI2HvvhdLjV14J7dsnHY2ISOmULBLkHga199xTixqJSPWmEuUJeu45eP11ePTRkDBERKortSwSsmZNuJ/iiCPgZz9LOhoRkR1TyyIhv/41fPYZTJigRY1EpPpTyyIBixfDPfdAv35wzDFJRyMikp6SRQKuuy5Uk/3Nb5KOREQkM+qGqmJTpsDLL4flUvfZJ+loREQyo5ZFFVq/HgYPhoMPhmuuSToaEZHMqWVRhUaNCuMVU6eGtbVFRGoKtSyqyGefwf/+L5x1FpxyStLRiIiUjZJFFbn+eigqgnvvTToSEZGyU7KoAq+/DuPGhYSx//5JRyMiUnZKFjErXtSoeXO44YakoxERKR8NcMds9OhQWXbCBNh116SjEREpH7UsYvTFF/DLX0KPHtCnT9LRiIiUX6zJwsx6mtlCM1tsZtt0wphZCzObbmZzzWymmeVG2zuY2VtmNi967bw444zLL38Jq1drUSMRqfliSxZmVhd4GDgVaAtcYGZtS+w2EnjS3dsDw4G7ou1rgf9x90OBnsAoM9sjrljjMGdOKD0+aFBYLlVEpCaLs2XRGVjs7h+5+wbgWeCsEvu0BaZHj2cUv+7ui9z9w+jxcmAFkBNjrJXKPdyh3bQp3HFH0tGIiFRcnMmiGbAs5XlhtC3Ve8A50eNeQEMza5K6g5l1BuoDS0qewMwGmFm+meWvXLmy0gKvqHHj4I034K67YI8a1R4SESldnMmitF56L/F8KNDNzOYA3YDPgKLNBzDbB3gKuNjdN21zMPfR7p7n7nk5OdWj4fHNNzBsGOTlwcUXJx2NiEjliHPqbCHQPOV5LrA8dYeoi6k3gJntDpzj7quj542AvwC/dPe3Y4yzUt15J/z73zBpEtTRXDMRyRJxfpzNAlqbWSszqw+cD0xO3cHMmppZcQw3Ao9H2+sDkwiD38/HGGOlWrQolPO46CI46qikoxERqTyxJQt3LwKuAqYBC4AJ7j7PzIab2ZnRbt2BhWa2CNgbuDPa/hPgeOAiMyuIfjrEFWtlcIdrr4UGDcJYhYhINjH3ksMINVNeXp7n5+cndv6XX4YzzgjLpQ4ZklgYIiJlYmaz3T0v3X7qVa8E69aFVsUhh4Q6UCIi2Ua1oSrBvffCkiXwyitQr17S0YiIVD61LCqosDDMgOrVC04+OeloRETioWRRQcOGwaZNWtRIRLKbkkUFvPoqPPss/OIX0LJl0tGIiMRHyaKciorCYHaLFiFZiIhkMw1wl9Ojj8L778PEibDLLklHIyISL7UsyuG//4VbboETT4TevZOORkQkfkoW5XDzzfD111rUSERqDyWLMnr3XfjDH8J4RduSSzmJiGQpJYsycA9JIicHbr896WhERKqOBrjL4Omn4f/+Dx5/HBo3TjoaEZGqo5ZFhr7+Gq6/Hjp3hv79k45GRKRqqWWRoV/9Cv7zH/jzn7WokYjUPvrYy8DChTBqFFxySWhZiIjUNkoWabjD4MHhxjstaiQitVWsycLMeprZQjNbbGY3lPJ6CzObbmZzzWymmeWmvDbVzL4ys5fjjDGdl16CadPgjjtgr72SjEREJDmxjVmYWV3gYeBkoBCYZWaT3X1+ym4jCetsjzWzE4C7gH7RayOAXYHL44oxnXXr4Lrrwv0UgwYlFYVI2Xz//fcUFhaybt26pEORaqRBgwbk5uZSr5yL7sQ5wN0ZWOzuHwGY2bPAWUBqsmgLXBc9ngG8WPyCu083s+4xxpfWyJHw0UcwfboWNZKao7CrVBlvAAANCklEQVSwkIYNG9KyZUtMJQYEcHe++OILCgsLadWqVbmOEWc3VDNgWcrzwmhbqveAc6LHvYCGZtYkxpgy9umn8OtfQ58+cMIJSUcjkrl169bRpEkTJQrZzMxo0qRJhVqbcSaL0v6leonnQ4FuZjYH6AZ8BhRlfAKzAWaWb2b5K1euLH+kpRg2LPx35MhKPaxIlVCikJIq+m8izmRRCDRPeZ4LLE/dwd2Xu3tvd+8I3BxtW53pCdx9tLvnuXteTk5OZcQMwIwZMGEC3HBDWK9CRGqXUaNGsXbt2tjPs3TpUg477DAA8vPzueaaawBYv349J510Eh06dOC5557j0ksvZf78+Ts61FZmzpzJ6aefXqmxxjlmMQtobWatCC2G84Gfpu5gZk2BVe6+CbgReDzGeDJSVATXXBNWvituXYhI1SkqKmKnnZK9X3jUqFH07duXXXfdtcrOmZeXR15eHgBz5szh+++/p6CgAIDzzjuvyuLYnthaFu5eBFwFTAMWABPcfZ6ZDTezM6PdugMLzWwRsDdwZ/H7zex14HngRDMrNLNT4oo11e9+B//6F9x3nxY1EimvpUuX0qZNG/r370/79u3p06cPa9euZfjw4Rx55JEcdthhDBgwAPfQM929e3duuukmunXrxv33389LL73EUUcdRceOHTnppJP4/PPPAbj99tvp378/P/rRj2jZsiUvvPAC119/Pe3ataNnz558//3328Qyc+ZMunfvTp8+fWjTpg0XXnjh5vNOnz6djh070q5dOy655BLWr1/PAw88wPLly+nRowc9evTY5nhPPPEEZ599NmeccQatWrXioYce4t5776Vjx4506dKFVatWAVBQUECXLl1o3749vXr14ssvvwRg9uzZHH744Rx99NE8/PDDW8V5+umns2LFCvr27UtBQQEdOnRgyZIldO/enfz8fABeeeUVjj76aI444gjOPfdc1qxZA8DUqVNp06YNxx57LC+88EJl/a/cwt2z4qdTp05eUStWuDdu7H7yye6bNlX4cCKJmD9//ubHgwe7d+tWuT+DB6eP4eOPP3bA33jjDXd3v/jii33EiBH+xRdfbN6nb9++PnnyZHd379atmw8cOHDza6tWrfJN0R/hH/7wBx8yZIi7u992223etWtX37BhgxcUFPguu+ziU6ZMcXf3s88+2ydNmrRNLDNmzPBGjRr5smXLfOPGjd6lSxd//fXX/bvvvvPc3FxfuHChu7v369fP77vvPnd3b9Giha9cubLUaxszZowfcMAB/vXXX/uKFSu8UaNG/sgjj7i7+7XXXrv5GO3atfOZM2e6u/stt9zig6NfXOr2oUOH+qGHHro5ztNOO22bx8W/n1mzZvnKlSv9uOOO8zVr1ri7+9133+133HHH5mtZtGiRb9q0yc8999yt3l8s9d9GMSDfM/iM1R3cKW66Cb79VosaiVSG5s2b07VrVwD69u3LG2+8wYwZMzjqqKNo164d//jHP5g3b97m/VO7WgoLCznllFNo164dI0aM2Gq/U089lXr16tGuXTs2btxIz549AWjXrh1Lly4tNZbOnTuTm5tLnTp16NChA0uXLmXhwoW0atWKgw46CID+/fvz2muvZXRtPXr0oGHDhuTk5NC4cWPOOOOMrWJYvXo1X331Fd26ddvq2CW39+vXb7vnKM3bb7/N/Pnz6dq1Kx06dGDs2LF88sknfPDBB7Rq1YrWrVtjZvTt27dMx82ECglG8vPhj3+EIUOgTZukoxGpHKNGJXfukrNvzIwrr7yS/Px8mjdvzu23377VVM7ddttt8+Orr76aIUOGcOaZZzJz5kxuT1lAZueddwagTp061KtXb/N56tSpQ1FREe+88w6XXx7u5R0+fDiNGjXa/B6AunXrUlRUtLkrKp1JkyZxxx13APDYY49tFUPxeVNjKira/oROd6/QrCR35+STT2b8+PFbbS8oKIh9BpxaFsCmTWFRo732gltvTToakezw6aef8tZbbwEwfvx4jj32WACaNm3KmjVrmDhx4nbfu3r1apo1C7dljR07tkznPeqooygoKKCgoIAzzzxzu/u1adOGpUuXsnjxYgCeeuqpzd/4GzZsyDfffANAr169Nh+veAA6ncaNG7Pnnnvy+uuvb3XsPfbYg8aNG/PGG28A8Mwzz5Tp2rp06cKbb765Oea1a9eyaNEi2rRpw8cff8ySJUsAtkkmlUEtC+Cpp+Dtt+GJJ6BRo6SjEckOhxxyCGPHjuXyyy+ndevWDBw4kC+//JJ27drRsmVLjjzyyO2+9/bbb+fcc8+lWbNmdOnShY8//rjS42vQoAFjxozh3HPPpaioiCOPPJIrrrgCgAEDBnDqqaeyzz77MGPGjHIdf+zYsVxxxRWsXbuW/fffnzFjxgAwZswYLrnkEnbddVdOOaVs83ZycnJ44oknuOCCC1i/fj0Av/rVrzjooIMYPXo0p512Gk2bNuXYY4/lX//6V7ni3h7LtClW3eXl5XnxbIGyWL0aDj4YWrWCN9/UWhVS8y1YsIBDDjkk0RiWLl3K6aefXukfWFIxpf3bMLPZ7p62yVTrPxq/+w6OPhoefFCJQkRke2p9N9QPfwiTJiUdhUh2admypVoVWUbfpUVEJC0lC5EslC1jkVJ5KvpvQslCJMs0aNCAL774QglDNvNoPYsGDRqU+xi1fsxCJNvk5uZSWFhIZZftl5qteKW88lKyEMky9erVK/dqaCLbo24oERFJS8lCRETSUrIQEZG0sqbch5mtBD6pwCGaAv+tpHCSlC3XAbqW6ipbriVbrgMqdi0t3D3tutRZkywqyszyM6mPUt1ly3WArqW6ypZryZbrgKq5FnVDiYhIWkoWIiKSlpLFFqOTDqCSZMt1gK6lusqWa8mW64AquBaNWYiISFpqWYiISFq1PlmY2eNmtsLManTxfTNrbmYzzGyBmc0zs8FJx1ReZtbAzP5pZu9F13JH0jFVhJnVNbM5ZvZy0rFUhJktNbP3zazAzMq+LGU1YmZ7mNlEM/sg+ps5OumYysPMDo7+fxT/fG1m18ZyrtreDWVmxwNrgCfd/bCk4ykvM9sH2Mfd3zWzhsBs4Gx3n59waGVmZgbs5u5rzKwe8AYw2N3fTji0cjGzIUAe0MjdT086nvIys6VAnrvX+HsTzGws8Lq7P2Zm9YFd3f2rpOOqCDOrC3wGHOXuFbnnrFS1vmXh7q8Bq5KOo6Lc/d/u/m70+BtgAdAs2ajKx4M10dN60U+N/FZjZrnAacBjSccigZk1Ao4H/gjg7htqeqKInAgsiSNRgJJFVjKzlkBH4J1kIym/qOumAFgB/M3da+q1jAKuBzYlHUglcOAVM5ttZgOSDqYC9gdWAmOi7sHHzGy3pIOqBOcD4+M6uJJFljGz3YE/Ade6+9dJx1Ne7r7R3TsAuUBnM6txXYRmdjqwwt1nJx1LJenq7kcApwKDoi7cmmgn4AjgEXfvCHwL3JBsSBUTdaWdCTwf1zmULLJI1L//J+AZd38h6XgqQ9Q9MBPomXAo5dEVODPq638WOMHMnk42pPJz9+XRf1cAk4DOyUZUboVAYUprdSIhedRkpwLvuvvncZ1AySJLRIPCfwQWuPu9ScdTEWaWY2Z7RI93AU4CPkg2qrJz9xvdPdfdWxK6CP7h7n0TDqtczGy3aOIEUZfNj4AaOYPQ3f8DLDOzg6NNJwI1biJICRcQYxcUaKU8zGw80B1oamaFwG3u/sdkoyqXrkA/4P2orx/gJnefkmBM5bUPMDaa3VEHmODuNXraaRbYG5gUvpOwEzDO3acmG1KFXA08E3XffARcnHA85WZmuwInA5fHep7aPnVWRETSUzeUiIikpWQhIiJpKVmIiEhaShYiIpKWkoWIiKSlZCFSDZhZ95pelVaym5KFiIikpWQhUgZm1jdaa6PAzB6NCh6uMbN7zOxdM5tuZjnRvh3M7G0zm2tmk8xsz2j7gWb292i9jnfN7IDo8LunrLHwTHRXvki1oGQhkiEzOwQ4j1BQrwOwEbgQ2I1Ql+cI4FXgtugtTwK/cPf2wPsp258BHnb3w4FjgH9H2zsC1wJtCZVRu8Z+USIZqvXlPkTK4ESgEzAr+tK/C6GE+ibguWifp4EXzKwxsIe7vxptHws8H9VXaubukwDcfR1AdLx/unth9LwAaElY+EkkcUoWIpkzYKy737jVRrNbSuy3oxo6O+paWp/yeCP6+5RqRN1QIpmbDvQxs70AzOwHZtaC8HfUJ9rnp8Ab7r4a+NLMjou29wNejdYYKTSzs6Nj7BwVghOp1vTNRSRD7j7fzH5JWC2uDvA9MIiweM6hZjYbWE0Y1wDoD/w+SgaplU37AY+a2fDoGOdW4WWIlIuqzopUkJmtcffdk45DJE7qhhIRkbTUshARkbTUshARkbSULEREJC0lCxERSUvJQkRE0lKyEBGRtJQsREQkrf8HRs8kP2akn+0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa376e05128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "param_not_modified_acc=[0.906,0.940,0.984,0.975,0.984,0.980,0.984]\n",
    "feature_set_random_acc=[0.953,0.950,0.969,0.960,0.980,0.985,0.975]\n",
    "# feature_set_zero_acc=[0.9,0.98,0.92,0.92,1,0.98,1]\n",
    "# feature_set_random_acc=[0.8,0.9,0.9,0.85,0.8,0.9,0.95]\n",
    "x = [1,2,3,4,5,6,7]#\n",
    "plt.plot(x,param_not_modified_acc,color = 'b',label=\"param-not-modified\")\n",
    "# plt.plot(x,feature_set_zero_acc,color = 'r',label=\"feature_set_zero\")\n",
    "# plt.plot(x,feature_set_random_acc,color = 'g',label=\"feature_set_random\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend(loc = \"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
